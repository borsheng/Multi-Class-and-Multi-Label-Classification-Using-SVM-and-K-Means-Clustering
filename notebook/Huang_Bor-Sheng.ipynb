{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78d64f7",
   "metadata": {},
   "source": [
    "## 1. Multi-class and Multi-Label Classification Using Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f32f06b",
   "metadata": {},
   "source": [
    "### (a) Download the Anuran Calls (MFCCs) Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eced00d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import statistics\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import hamming_loss, calinski_harabasz_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.spatial import distance\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7565a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Anuran Calls (MFCCs)/Frogs_MFCCs.csv')\n",
    "\n",
    "# label encoding\n",
    "le_f = preprocessing.LabelEncoder()\n",
    "le_g = preprocessing.LabelEncoder()\n",
    "le_s = preprocessing.LabelEncoder()\n",
    "df['Family'] = le_f.fit_transform(df['Family'])\n",
    "df['Genus'] = le_g.fit_transform(df['Genus'])\n",
    "df['Species'] = le_s.fit_transform(df['Species'])\n",
    "\n",
    "# separate features, labels and split the data\n",
    "X = df.iloc[:, :-4].to_numpy()\n",
    "y = df.iloc[:, -4:-1].to_numpy()\n",
    "train_feature, test_feature, train_label, test_label = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ae737",
   "metadata": {},
   "source": [
    "### (b) Each instance has three labels: Families, Genus, and Species. Each of the labels has multiple classes. We wish to solve a multi-class and multi-label problem. One of the most important approaches to multi-label classification is to train a classifier for each label (binary relevance). We first try this approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9cd727",
   "metadata": {},
   "source": [
    "#### i. Research exact match and hamming score/ loss methods for evaluating multi-label classification and use them in evaluating the classifiers in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f97cb9",
   "metadata": {},
   "source": [
    "Exact match accuracy calculates subset accuracy, requiring the predicted labels to precisely match the true labels.\n",
    "\n",
    "Hamming loss measures the proportion of incorrect labels to the total number of labels, providing a broader evaluation of label correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe2cdf5",
   "metadata": {},
   "source": [
    "#### ii. Train a SVM for each of the labels, using Gaussian kernels and one versus all classifiers. Determine the weight of the SVM penalty and the width of the Gaussian Kernel using 10 fold cross validation. You are welcome to try to solve the problem with both standardized and raw attributes and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0716fbb2",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Family label: loss = 0.00635275647701095, C = 10.0, and gamma = 1.9\n",
      "For Genus label: loss = 0.008936855060115498, C = 10.0, and gamma = 1.7\n",
      "For Species label: loss = 0.009135267758528196, C = 10.0, and gamma = 1.5\n",
      "Testing:\n",
      "For Family label: test loss = 0.00555812876331635\n",
      "For Genus label: test loss = 0.009263547938860583\n",
      "For Species label: test loss = 0.009263547938860583\n"
     ]
    }
   ],
   "source": [
    "# optimize hyperparameters\n",
    "def optimize_hyperparameters(train_feature, train_label, alphas, gammas):\n",
    "    best_loss = 1\n",
    "    best_alpha = 0\n",
    "    best_gamma = 0\n",
    "\n",
    "    for alpha in alphas:\n",
    "        for gamma in gammas:\n",
    "            loss = 0\n",
    "            kf = KFold(n_splits=10)\n",
    "\n",
    "            for train_index, test_index in kf.split(train_feature):\n",
    "                X_train, X_test = train_feature[train_index], train_feature[test_index]\n",
    "                y_train, y_test = train_label[train_index], train_label[test_index]\n",
    "\n",
    "                clf = SVC(kernel='rbf', random_state=42, gamma=gamma, C=alpha)\n",
    "                ovr = OneVsRestClassifier(clf)\n",
    "                ovr.fit(X_train, y_train)\n",
    "\n",
    "                predicted = ovr.predict(X_test)\n",
    "                loss += hamming_loss(y_test, predicted)\n",
    "\n",
    "            if best_loss > loss / 10:\n",
    "                best_loss = loss / 10\n",
    "                best_alpha = alpha\n",
    "                best_gamma = gamma\n",
    "\n",
    "    return best_loss, best_alpha, best_gamma\n",
    "\n",
    "alphas = np.logspace(1, 10, 10) / 10000\n",
    "gammas = [x / 10.0 for x in range(1, 21, 2)]\n",
    "\n",
    "f_loss, f_alpha, f_gamma = optimize_hyperparameters(train_feature, train_label[:, 0], alphas, gammas)\n",
    "g_loss, g_alpha, g_gamma = optimize_hyperparameters(train_feature, train_label[:, 1], alphas, gammas)\n",
    "s_loss, s_alpha, s_gamma = optimize_hyperparameters(train_feature, train_label[:, 2], alphas, gammas)\n",
    "\n",
    "print(f'For Family label: loss = {f_loss}, C = {f_alpha}, and gamma = {f_gamma}')\n",
    "print(f'For Genus label: loss = {g_loss}, C = {g_alpha}, and gamma = {g_gamma}')\n",
    "print(f'For Species label: loss = {s_loss}, C = {s_alpha}, and gamma = {s_gamma}')\n",
    "\n",
    "# testing\n",
    "def evaluate_test_loss(X_train, y_train, X_test, y_test, gamma, C):\n",
    "    clf = SVC(kernel='rbf', random_state=42, gamma=gamma, C=C)\n",
    "    ovr = OneVsRestClassifier(clf)\n",
    "    ovr.fit(X_train, y_train)\n",
    "    predicted = ovr.predict(X_test)\n",
    "    return hamming_loss(y_test, predicted)\n",
    "\n",
    "f_test_loss = evaluate_test_loss(train_feature, train_label[:, 0], test_feature, test_label[:, 0], f_gamma, f_alpha)\n",
    "g_test_loss = evaluate_test_loss(train_feature, train_label[:, 1], test_feature, test_label[:, 1], g_gamma, g_alpha)\n",
    "s_test_loss = evaluate_test_loss(train_feature, train_label[:, 2], test_feature, test_label[:, 2], s_gamma, s_alpha)\n",
    "\n",
    "print('Testing:')\n",
    "print(f'For Family label: test loss = {f_test_loss}')\n",
    "print(f'For Genus label: test loss = {g_test_loss}')\n",
    "print(f'For Species label: test loss = {s_test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258bb058",
   "metadata": {},
   "source": [
    "#### iii. Repeat 1(b)ii with L1-penalized SVMs. Remember to standardize the attributes. Determine the weight of the SVM penalty using 10 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2534d7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Family label: loss = 0.05937075956956672 and C = 10.0\n",
      "For Genus label: loss = 0.04765533781438354 and C = 10.0\n",
      "For Species label: loss = 0.039714096374136136 and C = 10.0\n",
      "Testing:\n",
      "For Family label: test loss = 0.07271885132005558\n",
      "For Genus label: test loss = 0.05789717461787865\n",
      "For Species label: test loss = 0.03890690134321445\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
    "\n",
    "# standardize the features\n",
    "scaler = StandardScaler()\n",
    "std_train_feature = scaler.fit_transform(train_feature)\n",
    "\n",
    "# optimize hyperparameters for L1-penalized SVM\n",
    "def optimize_hyperparameters_l1(train_feature, train_label, alphas):\n",
    "    best_loss = 1\n",
    "    best_alpha = 0\n",
    "\n",
    "    for alpha in alphas:\n",
    "        loss = 0\n",
    "        kf = KFold(n_splits=10)\n",
    "\n",
    "        for train_index, test_index in kf.split(train_feature):\n",
    "            X_train, X_test = train_feature[train_index], train_feature[test_index]\n",
    "            y_train, y_test = train_label[train_index], train_label[test_index]\n",
    "\n",
    "            clf = LinearSVC(penalty='l1', random_state=42, C=alpha, verbose=0, max_iter=10000, dual=False)\n",
    "            ovr = OneVsRestClassifier(clf)\n",
    "            ovr.fit(X_train, y_train)\n",
    "\n",
    "            predicted = ovr.predict(X_test)\n",
    "            loss += hamming_loss(y_test, predicted)\n",
    "\n",
    "        if best_loss > loss / 10:\n",
    "            best_loss = loss / 10\n",
    "            best_alpha = alpha\n",
    "\n",
    "    return best_loss, best_alpha\n",
    "\n",
    "alphas = np.logspace(1, 10, 10) / 10000\n",
    "f_loss, f_alpha = optimize_hyperparameters_l1(std_train_feature, train_label[:, 0], alphas)\n",
    "g_loss, g_alpha = optimize_hyperparameters_l1(std_train_feature, train_label[:, 1], alphas)\n",
    "s_loss, s_alpha = optimize_hyperparameters_l1(std_train_feature, train_label[:, 2], alphas)\n",
    "\n",
    "print(f'For Family label: loss = {f_loss} and C = {f_alpha}')\n",
    "print(f'For Genus label: loss = {g_loss} and C = {g_alpha}')\n",
    "print(f'For Species label: loss = {s_loss} and C = {s_alpha}')\n",
    "\n",
    "# testing\n",
    "def evaluate_test_loss_l1(X_train, y_train, X_test, y_test, C):\n",
    "    clf = LinearSVC(penalty='l1', random_state=42, C=C, verbose=0, max_iter=10000, dual=False)\n",
    "    ovr = OneVsRestClassifier(clf)\n",
    "    ovr.fit(X_train, y_train)\n",
    "    predicted = ovr.predict(X_test)\n",
    "    return hamming_loss(y_test, predicted)\n",
    "\n",
    "f_test_loss = evaluate_test_loss_l1(train_feature, train_label[:, 0], test_feature, test_label[:, 0], f_alpha)\n",
    "g_test_loss = evaluate_test_loss_l1(train_feature, train_label[:, 1], test_feature, test_label[:, 1], g_alpha)\n",
    "s_test_loss = evaluate_test_loss_l1(train_feature, train_label[:, 2], test_feature, test_label[:, 2], s_alpha)\n",
    "\n",
    "print('Testing:')\n",
    "print(f'For Family label: test loss = {f_test_loss}')\n",
    "print(f'For Genus label: test loss = {g_test_loss}')\n",
    "print(f'For Species label: test loss = {s_test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00edf05",
   "metadata": {},
   "source": [
    "#### iv. Repeat iii. by using SMOTE or any other method you know to remedy class imbalance. Report your conclusions about the classifiers you trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee8f99e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Family label with SMOTE: loss = 0.055234608082451855 and C = 10.0\n",
      "For Genus label with SMOTE: loss = 0.07476453881726666 and C = 100.0\n",
      "For Species label with SMOTE: loss = 0.16142214957090315 and C = 1000.0\n",
      "Testing with SMOTE:\n",
      "For Family label: test loss = 0.08985641500694766\n",
      "For Genus label: test loss = 0.0856878184344604\n",
      "For Species label: test loss = 0.04353867531264474\n"
     ]
    }
   ],
   "source": [
    "# apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_f, y_train_f = smote.fit_resample(std_train_feature, train_label[:,0])\n",
    "X_train_g, y_train_g = smote.fit_resample(std_train_feature, train_label[:,1])\n",
    "X_train_s, y_train_s = smote.fit_resample(std_train_feature, train_label[:,2])\n",
    "\n",
    "f_loss, f_alpha = optimize_hyperparameters_l1(X_train_f, y_train_f, alphas)\n",
    "g_loss, g_alpha = optimize_hyperparameters_l1(X_train_g, y_train_g, alphas)\n",
    "s_loss, s_alpha = optimize_hyperparameters_l1(X_train_s, y_train_s, alphas)\n",
    "\n",
    "print(f'For Family label with SMOTE: loss = {f_loss} and C = {f_alpha}')\n",
    "print(f'For Genus label with SMOTE: loss = {g_loss} and C = {g_alpha}')\n",
    "print(f'For Species label with SMOTE: loss = {s_loss} and C = {s_alpha}')\n",
    "\n",
    "# testing\n",
    "std_test_feature = scaler.fit_transform(test_feature)\n",
    "\n",
    "f_test_loss = evaluate_test_loss_l1(X_train_f, y_train_f, std_test_feature, test_label[:, 0], f_alpha)\n",
    "g_test_loss = evaluate_test_loss_l1(X_train_g, y_train_g, std_test_feature, test_label[:, 1], g_alpha)\n",
    "s_test_loss = evaluate_test_loss_l1(X_train_s, y_train_s, std_test_feature, test_label[:, 2], s_alpha)\n",
    "\n",
    "print('Testing with SMOTE:')\n",
    "print(f'For Family label: test loss = {f_test_loss}')\n",
    "print(f'For Genus label: test loss = {g_test_loss}')\n",
    "print(f'For Species label: test loss = {s_test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6025c5c",
   "metadata": {},
   "source": [
    "Conclusions:   \n",
    "In this problem, Support Vector Machines (SVMs) were employed to classify anuran calls into family, genus, and species labels. Gaussian kernels yielded optimal performance with penalties (C) set to 10.0 and kernel widths (gamma) of 1.9, 1.7, and 1.5 for family, genus, and species labels, respectively. The L1-penalized SVMs, after attribute standardization, exhibited losses of 0.059, 0.048, and 0.040 for family, genus, and species labels, with C values fixed at 10.0. The introduction of Synthetic Minority Over-sampling Technique (SMOTE) to address class imbalance resulted in varied C values (10.0, 100.0, 1000.0) and increased testing losses. Overall, the models demonstrate effective label classification, with some sensitivity to class imbalance mitigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce88921f",
   "metadata": {},
   "source": [
    "### 2. K-Means Clustering on a Multi-Class and Multi-Label Data Set. Monte-Carlo Simulation: Perform the following procedures 50 times, and report the average and standard deviation of the 50 Hamming Distances that you calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871512f8",
   "metadata": {},
   "source": [
    "#### (a) Use k-means clustering on the whole Anuran Calls (MFCCs) Data Set (do not split the data into train and test, as we are not performing supervised learning in this exercise). Choose k ∈ {1, 2, . . . , 50} automatically based on one of the methods provided in the slides (CH or Gap Statistics or scree plots or Silhouettes) or any other method you know.\n",
    "#### (b) In each cluster, determine which family is the majority by reading the true labels. Repeat for genus and species.\n",
    "#### (c) Now for each cluster you have a majority label triplet (family, genus, species). Calculate the average Hamming distance, Hamming score, and Hamming loss between the true labels and the labels assigned by clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8792ede4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Hamming Distance: 0.895621959694232\n",
      "Average Hamming Score: 0.7014593467685891\n",
      "Average Hamming Loss: 0.29854065323141077\n",
      "Standard Deviation of Hamming Distance: 1.1102230246251565e-16\n",
      "Standard Deviation of Hamming Score: 1.1102230246251565e-16\n",
      "Standard Deviation of Hamming Loss: 5.551115123125783e-17\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/Anuran Calls (MFCCs)/Frogs_MFCCs.csv')\n",
    "raw_data = df.iloc[:, :-4].to_numpy()\n",
    "\n",
    "hamming_distances = []\n",
    "hamming_scores = []\n",
    "hamming_losses = []\n",
    "\n",
    "num_iterations = 50\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # (a): use k-means clustering\n",
    "    results = {}\n",
    "    for i in range(2, 51):\n",
    "        kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "        predicted = kmeans.fit_predict(raw_data)\n",
    "        index = calinski_harabasz_score(raw_data, predicted)\n",
    "        results[i] = index\n",
    "\n",
    "    best_k = max(results, key=results.get)\n",
    "\n",
    "    # (b): determine which family is majority\n",
    "    kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
    "    predicted = kmeans.fit_predict(raw_data)\n",
    "\n",
    "    majority_labels = []\n",
    "    for label in range(best_k):\n",
    "        cluster_mask = (predicted == label)\n",
    "        majority_labels.append({\n",
    "            'Family': df['Family'][cluster_mask].mode().values[0],\n",
    "            'Genus': df['Genus'][cluster_mask].mode().values[0],\n",
    "            'Species': df['Species'][cluster_mask].mode().values[0]\n",
    "        })\n",
    "\n",
    "    # (c): calculate the average Hamming distance, score, and loss\n",
    "    df['Family_pre'] = [majority_labels[label]['Family'] for label in predicted]\n",
    "    df['Genus_pre'] = [majority_labels[label]['Genus'] for label in predicted]\n",
    "    df['Species_pre'] = [majority_labels[label]['Species'] for label in predicted]\n",
    "\n",
    "    true_labels = df[['Family', 'Genus', 'Species']].to_numpy()\n",
    "    predicted_labels = df[['Family_pre', 'Genus_pre', 'Species_pre']].to_numpy()\n",
    "    \n",
    "    hamming_distances.append(np.sum(np.not_equal(true_labels, predicted_labels))/float(predicted_labels.size)*3)\n",
    "    hamming_scores.append(1-np.sum(np.not_equal(true_labels, predicted_labels))/float(predicted_labels.size))\n",
    "    hamming_losses.append(np.sum(np.not_equal(true_labels, predicted_labels))/float(predicted_labels.size))\n",
    "\n",
    "# calculate average and standard deviation\n",
    "avg_hamming_distance = np.mean(hamming_distances)\n",
    "avg_hamming_score = np.mean(hamming_scores)\n",
    "avg_hamming_loss = np.mean(hamming_losses)\n",
    "std_dev_hamming_distance = np.std(hamming_distances)\n",
    "std_dev_hamming_score = np.std(hamming_scores)\n",
    "std_dev_hamming_loss = np.std(hamming_losses)\n",
    "\n",
    "print(f\"Average Hamming Distance: {avg_hamming_distance}\")\n",
    "print(f\"Average Hamming Score: {avg_hamming_score}\")\n",
    "print(f\"Average Hamming Loss: {avg_hamming_loss}\")\n",
    "print(f\"Standard Deviation of Hamming Distance: {std_dev_hamming_distance}\")\n",
    "print(f\"Standard Deviation of Hamming Score: {std_dev_hamming_score}\")\n",
    "print(f\"Standard Deviation of Hamming Loss: {std_dev_hamming_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b9716a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cb5b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
